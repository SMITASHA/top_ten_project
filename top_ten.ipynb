{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.parse as UP\n",
    "import yaml\n",
    "import pymongo\n",
    "import bs4\n",
    "import re\n",
    "import numpy as np\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up path for chromedriver\n",
    "\n",
    "with open(\"config.yml\", \"r\") as ymlpath:\n",
    "    config = yaml.safe_load(ymlpath)\n",
    "    executable_path = {\"executable_path\": config[\"config-key\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOR TESTING PURPOSES: For windows, un-comment cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executable_path = {\"executable_path\": \"chromedriver\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Box Office Mojo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_scraper():\n",
    "    \n",
    "    \"\"\"Scrapes www.boxofficemojo.com for the top ten movies for 2018 based on gross box-office amount.\n",
    "    Returns a list of dictionaries with year, rank, movie title, and studio\"\"\"\n",
    "    \n",
    "    year = str(2018)\n",
    "    movie_df_list=[]\n",
    "    \n",
    "    # Get webpage data using requests and parse html by creating a beautiful soup object\n",
    "    response = requests.get(\"https://www.boxofficemojo.com/yearly/chart/?view2=worldwide&yr=%s&p=.htm\" % year)\n",
    "    soup = bs(response.text,\"html.parser\")\n",
    "\n",
    "    # Find location of necessary data in soup object\n",
    "    soup_tables = soup.find_all(\"table\")\n",
    "    soup_elements = soup_tables[3].find_all(\"td\")\n",
    "\n",
    "    # For each td element, find and store data in a list \n",
    "    movie_data=[]\n",
    "\n",
    "    for i in soup_elements:\n",
    "        if i.find(\"a\")!=None:\n",
    "            movie_data.append(i.find(\"a\").contents[0]) \n",
    "        elif i.find(\"font\")!=None:\n",
    "            movie_data.append(i.find(\"font\").contents[0])\n",
    "        elif i.find(\"b\")!=None:\n",
    "            movie_dataappend(i.find(\"b\").contents[0])\n",
    "\n",
    "    ### Clean Data:\n",
    "\n",
    "    # Remove extraneous tags\n",
    "    movie_data = [a.contents[0] if type(a)!=bs4.element.NavigableString else a for a in movie_data]\n",
    "\n",
    "    # Strip special characters\n",
    "    movie_data = [re.sub(\"[^A-Za-z0-9-. ]+\", \"\", a) for a in movie_data]\n",
    "\n",
    "    # Fill NaNs\n",
    "    movie_data = [np.nan if a ==\"na\" else a for a in movie_data]\n",
    "\n",
    "    # Set first 6 elements as column headers\n",
    "    to_df = movie_data[6:]\n",
    "\n",
    "    # Define the column names \n",
    "    columns = [\"rank\",\"title\",\"studio\",\"worldwide-gross\",\"domestic-gross\",\"domestic-pct\",\"overseas-gross\",\"overseas-pct\"]\n",
    "\n",
    "    # Convert to dataframe\n",
    "    nrow = int(len(to_df)/len(columns)) \n",
    "    dirty_movies_df = pd.DataFrame(np.array(to_df).reshape(nrow,8),columns=columns)\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    dirty_movies_df = dirty_movies_df.iloc[: , 0:3]\n",
    "    dirty_movies_df[\"rank\"] = dirty_movies_df[\"rank\"].apply(int)\n",
    "    movies_df = dirty_movies_df.loc[dirty_movies_df[\"rank\"] <=10,:]\n",
    "    \n",
    "    # Convert dataframe to list of dictionaries\n",
    "    movie_dicts = movies_df.to_dict(orient=\"records\") \n",
    "    \n",
    "    print(\"Movies Scraped from BoxOfficeMojo.\")\n",
    "    \n",
    "    return (movie_dicts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Billboard Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chart(data, year):\n",
    "    \n",
    "    \"\"\" Use the Python package for parsing HTML.  Calls and receives HTML as strings to process for artists.\"\"\"\n",
    "    \n",
    "    # Create soup object to parse the html\n",
    "    soup = bs(data,\"html5lib\")\n",
    "    \n",
    "    # Create a list to return\n",
    "    list_albums = []\n",
    "\n",
    "    # Inspect parsed html\n",
    "    # For each article item, loop and identify tags to extract from.\n",
    "    # For each entry, add a dictionary to the album list\n",
    "    \n",
    "    for item in soup.select(\"article\"):\n",
    "        rank = int(item.select_one(\".ye-chart-item__rank\").string.strip())\n",
    "        title = item.select_one(\".ye-chart-item__title\").string.strip()\n",
    "        artist = item.select_one(\".ye-chart-item__artist\").text.replace(\"\\n\", \"\")\n",
    "        list_albums.append({\"rank\":rank, \"title\":title, \"artist\":artist,\" year\":year})\n",
    "    \n",
    "    return(list_albums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def album_scraper():\n",
    "\n",
    "    \"\"\"Scrapes www.billboard.com for the top ten albums for 2008-2018 based on gross box-office amount.\n",
    "    Returns a list of dictionaries with year, album title, and artist name\"\"\"\n",
    "\n",
    "    # Create a list of years we are querying data for\n",
    "    year = str(2018)\n",
    "    \n",
    "    all_albums = []\n",
    "\n",
    "    # For each year, use requests library to get HTML and parse contentus using process_chart function\n",
    "    # Add newly created list of dictionaries for specified year to comprehensive list for all years\n",
    "    url = requests.get(\"https://www.billboard.com/charts/year-end/\"+str(year)+\"/top-billboard-200-albums\")\n",
    "    data = url.content\n",
    "    all_albums = process_chart(data,year)\n",
    "    \n",
    "    # Filter just the top 10 albums for each year and insert into final list of dictionaries\n",
    "    album_dicts = []\n",
    "    for album in all_albums:\n",
    "        if (album[\"rank\"] < 11):\n",
    "            album_dicts.append(album)\n",
    "            \n",
    "    print(\"Albums Scraped from Billboard.\")\n",
    "    \n",
    "    return(album_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Metacritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metacritic_movie_scraper(url):\n",
    "\n",
    "    \"\"\"Scrapes given metacritic.com url for the movie review information.\n",
    "    Returns a dictionary with number of user reviews, average user review, number of critic reviews, and critic score\"\"\"\n",
    "    \n",
    "    # Use splinter and beautiful soup to parse given url\n",
    "    with Browser(\"chrome\", **executable_path, headless=True) as browser:\n",
    "        browser.visit(url)\n",
    "        soup = bs(browser.html, \"html.parser\")\n",
    "\n",
    "    try:\n",
    "        # Find number of reviews from users and critics\n",
    "        rev_count_strings = soup.find_all(\"span\", class_=\"based_on\")\n",
    "        user_rev_count = int(rev_count_strings[1].text.split(\" \")[2])\n",
    "        critic_rev_count = int(rev_count_strings[0].text.split(\" \")[2])\n",
    "\n",
    "        # Find review average from users and rating score from critics\n",
    "        review_soup = soup.find_all(\"a\", class_=\"metascore_anchor\")\n",
    "        user_rev_avg = float(review_soup[1].text)\n",
    "        critic_rev_score = int(review_soup[0].text)\n",
    "\n",
    "    # If page does note have review information, population review information with None values\n",
    "    except (IndexError, AttributeError):\n",
    "        user_rev_count = None\n",
    "        critic_rev_count = None\n",
    "        user_rev_avg = None\n",
    "        critic_rev_score = None    \n",
    "    \n",
    "    # Return dictionary of book information\n",
    "    movie_dict = {\"user_rev_count\": user_rev_count, \"user_rev_avg\": user_rev_avg, \"critic_rev_count\": critic_rev_count, \"critic_rev_score\": critic_rev_score}\n",
    "\n",
    "    return(movie_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metacritic_album_scraper(url):\n",
    "\n",
    "    \"\"\"Scrapes given metacritic.com url for the album review information.\n",
    "    Returns a dictionary with number of user reviews, average user review, number of critic reviews, and critic score\"\"\"\n",
    "    \n",
    "    # Use splinter and beautiful soup to parse given url   \n",
    "    with Browser(\"chrome\", **executable_path, headless=True) as browser:\n",
    "        browser.visit(url)\n",
    "        soup = bs(browser.html, \"html.parser\")\n",
    "    \n",
    "    try:\n",
    "        # Find review average from users and rating score from critics\n",
    "        review_soup = soup.find_all(\"a\", class_=\"metascore_anchor\")\n",
    "        user_rev_avg = float(review_soup[1].text)\n",
    "        critic_rev_score = int(review_soup[0].text)\n",
    "\n",
    "        # Find number of user reviews\n",
    "        count_soup = soup.find(\"div\",class_=\"module reviews_module user_reviews_module\")\n",
    "        user_rev_count_string = count_soup.find(\"span\",class_=\"count\")\n",
    "        user_rev_count = int(user_rev_count_string.text)\n",
    "\n",
    "        # Find number of critic reviews\n",
    "        critic_rev_count_string = count_soup.find(\"span\",class_=\"count\")\n",
    "        critic_rev_count = int(critic_rev_count_string.text)\n",
    "    \n",
    "    # If page does note have review information, population review information with None values\n",
    "    except (IndexError, AttributeError) :\n",
    "        user_rev_count = None\n",
    "        critic_rev_count = None\n",
    "        user_rev_avg = None\n",
    "        critic_rev_score = None\n",
    "\n",
    "    # Return dictionary of album information\n",
    "    album_dict = {\"user_rev_count\": user_rev_count, \"user_rev_avg\": user_rev_avg, \"critic_rev_count\": critic_rev_count, \"critic_rev_score\": critic_rev_score}\n",
    "  \n",
    "    return (album_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_url_string(string):\n",
    "    \"\"\"Takes a string and returns a string to be inserted in url\"\"\"\n",
    "    \n",
    "    url_string = string.replace(\"(\", \"\").replace(\")\",\"\").replace(\"÷\", \"\").replace(\"&\", \"\").replace(\"-\", \"\").\\\n",
    "    replace(\"  \", \" \").replace(\" \", \"-\").lower()\n",
    "    \n",
    "    if url_string.startswith(\"-\"):\n",
    "        url_string = url_string[1:]\n",
    "    \n",
    "    if url_string.endswith(\"-\"):\n",
    "        url_string = url_string[: -1]\n",
    "\n",
    "    return(url_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create list of dictionaries for top movies and music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies Scraped from BoxOfficeMojo.\n",
      "Albums Scraped from Billboard.\n"
     ]
    }
   ],
   "source": [
    "# Scrape BoxOfficeMojo and Billboard Music for a list of dictionaries of the top 10 movies for 2008-2018\n",
    "movie_BOM_dicts = movie_scraper()\n",
    "album_Bill_dicts = album_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avengers Infinity War scraped\n",
      "Black Panther scraped\n",
      "Jurassic World Fallen Kingdom scraped\n",
      "Incredibles 2 scraped\n",
      "Aquaman scraped\n",
      "Bohemian Rhapsody scraped\n",
      "Venom 2018 scraped\n",
      "Mission Impossible - Fallout scraped\n",
      "Deadpool 2 scraped\n",
      "Fantastic Beasts The Crimes of Grindelwald scraped\n",
      "reputation scraped\n",
      "Scorpion scraped\n",
      "beerbongs & bentleys scraped\n",
      "The Greatest Showman scraped\n",
      "÷ (Divide) scraped\n",
      "Invasion Of Privacy scraped\n",
      "ASTROWORLD scraped\n",
      "Stoney scraped\n",
      "? scraped\n",
      "Culture II scraped\n"
     ]
    }
   ],
   "source": [
    "# Add review information from Metacritic to new list of dictionaries for top movies\n",
    "movie_dicts = []\n",
    "for movie in movie_BOM_dicts:\n",
    "    \n",
    "    # Create query url from dictionary values\n",
    "    movie_query = make_url_string(movie[\"title\"])\n",
    "    movie_url = f\"https://www.metacritic.com/movie/{movie_query}/details\"\n",
    "\n",
    "    # Add review information to dictionary\n",
    "    movie_dicts.append({**movie, **metacritic_movie_scraper(movie_url)})\n",
    "    print(f\"{movie['title']} scraped\")\n",
    "    \n",
    "# Add review information from Metacritic to new list of dictionaries for top music albums\n",
    "album_dicts = []\n",
    "for album in album_Bill_dicts:\n",
    "    # Create query url from dictionary values\n",
    "    title_query = make_url_string(album[\"title\"])\n",
    "    artist_query = make_url_string(album[\"artist\"])\n",
    "    album_url = f\"https://www.metacritic.com/music/{title_query}/{artist_query}\"\n",
    "    \n",
    "    # Add review information to dictionary\n",
    "    album_dicts.append({**album, **metacritic_album_scraper(album_url)})\n",
    "    print(f\"{album['title']} scraped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate mongo database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x11b2f39c8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to mongo using pymongo to create local database\n",
    "conn = \"mongodb://localhost:27017\"\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "# Create Top 10 database\n",
    "db = client.top_10_db\n",
    "\n",
    "# Create movies and albums collections\n",
    "movies = db.movies\n",
    "albums = db.albums\n",
    "\n",
    "# Insert top 10 movies and albums for 2008-2018\n",
    "# GRETEL - FIGURE OUT WHETHER WE WANT TO UPSERT\n",
    "db.movies.insert_many(movie_dicts)\n",
    "db.albums.insert_many(album_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
