{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dependencies\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.parse as UP\n",
    "import yaml\n",
    "import pymongo\n",
    "import bs4\n",
    "import re\n",
    "import numpy as np\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up path for chromedriver\n",
    "\n",
    "with open(\"config.yml\", 'r') as ymlpath:\n",
    "    config = yaml.safe_load(ymlpath)\n",
    "    executable_path = {\"executable_path\": config[\"config-key\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Box Office Mojo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_scraper():\n",
    "    \n",
    "    \"\"\"Scrapes www.boxofficemojo.com for the top ten movies for 2008-2018 based on gross box-office amount.\n",
    "    Returns a list of dictionaries with year, rank, movie title, and studio\"\"\"\n",
    "    \n",
    "    # Create a list of year we are querying data for\n",
    "    years = [str(a) for a in range(2008,2019)]\n",
    "    \n",
    "    movie_df_list=[]\n",
    "    \n",
    "    for year in years:\n",
    "        \n",
    "        # Get webpage data using requests and parse html by creating a beautiful soup object\n",
    "        response = requests.get('https://www.boxofficemojo.com/yearly/chart/?view2=worldwide&yr=%s&p=.htm' % year)\n",
    "        soup = bs(response.text,'html.parser')\n",
    "\n",
    "        # Find location of necessary data in soup object\n",
    "        soup_tables = soup.find_all('table')\n",
    "        soup_elements = soup_tables[3].find_all('td')\n",
    "\n",
    "        # For each td element, find and store data in a list \n",
    "        movie_data=[]\n",
    "        \n",
    "        for i in soup_elements:\n",
    "            if i.find('a')!=None:\n",
    "                movie_data.append(i.find('a').contents[0]) \n",
    "            elif i.find('font')!=None:\n",
    "                movie_data.append(i.find('font').contents[0])\n",
    "            elif i.find('b')!=None:\n",
    "                movie_dataappend(i.find('b').contents[0])\n",
    "\n",
    "        ### Clean Data:\n",
    "        \n",
    "        # Remove extraneous tags\n",
    "        movie_data = [a.contents[0] if type(a)!=bs4.element.NavigableString else a for a in movie_data]\n",
    "\n",
    "        # Strip special characters\n",
    "        movie_data = [re.sub('[^A-Za-z0-9-. ]+', '', a) for a in movie_data]\n",
    "\n",
    "        # Fill NaNs\n",
    "        movie_data = [np.nan if a =='na' else a for a in movie_data]\n",
    "        \n",
    "        # Set first 6 elements as column headers\n",
    "        to_df = movie_data[6:]\n",
    "\n",
    "        # Define the column names \n",
    "        columns = ['rank','title','studio','worldwide-gross','domestic-gross','domestic-pct','overseas-gross','overseas-pct']\n",
    "\n",
    "        # Convert to dataframe\n",
    "        nrow = int(len(to_df)/len(columns)) \n",
    "        dirty_movies_df = pd.DataFrame(np.array(to_df).reshape(nrow,8),columns=columns)\n",
    "        \n",
    "        # Remove unnecessary columns\n",
    "        dirty_movies_df = dirty_movies_df.iloc[: , 0:3]\n",
    "        dirty_movies_df[\"rank\"] = dirty_movies_df[\"rank\"].apply(int)\n",
    "        dirty_movies_df = dirty_movies_df.loc[dirty_movies_df[\"rank\"] <=10,:]\n",
    "        \n",
    "        # Add year column to dataframe\n",
    "        dirty_movies_df['year']=int(year)\n",
    "        \n",
    "        # Add dataframe for specified year to list of dataframes for all years\n",
    "        movie_df_list.append(dirty_movies_df)\n",
    "        \n",
    "    # Convert list of dataframes to single dataframe\n",
    "    movie_df = pd.concat(movie_df_list)\n",
    "    movie_dicts = movie_df.to_dict(orient='records') \n",
    "    \n",
    "    print(\"Movies Scraped from BoxOfficeMojo.\")\n",
    "    \n",
    "    return (movie_dicts) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Billboard Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chart(data, year):\n",
    "    \n",
    "    \"\"\" Use the Python package for parsing HTML.  Calls and receives HTML as strings to process for artists.\"\"\"\n",
    "    \n",
    "    # Create soup object to parse the html\n",
    "    soup = bs(data,\"html5lib\")\n",
    "    \n",
    "    # Create a list to return\n",
    "    list_albums = []\n",
    "\n",
    "    # Inspect parsed html\n",
    "    # For each article item, loop and identify tags to extract from.\n",
    "    # For each entry, add a dictionary to the album list\n",
    "    \n",
    "    for item in soup.select('article'):\n",
    "        rank = int(item.select_one(\".ye-chart-item__rank\").string.strip())\n",
    "        title = item.select_one(\".ye-chart-item__title\").string.strip()\n",
    "        artist = item.select_one(\".ye-chart-item__artist\").text.replace(\"\\n\", \"\")\n",
    "        list_albums.append({'rank':rank, 'title':title, 'artist':artist,' year':year})\n",
    "    \n",
    "    return(list_albums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def album_scraper():\n",
    "\n",
    "    \"\"\"Scrapes www.billboard.com for the top ten albums for 2008-2018 based on gross box-office amount.\n",
    "    Returns a list of dictionaries with year, album title, and artist name\"\"\"\n",
    "\n",
    "    # Create a list of years we are querying data for\n",
    "    years = [str(a) for a in range(2008,2019)]\n",
    "    \n",
    "    all_albums = []\n",
    "\n",
    "    # For each year, use requests library to get HTML and parse contentus using process_chart function\n",
    "    # Add newly created list of dictionaries for specified year to comprehensive list for all years\n",
    "    for year in years:\n",
    "        url = requests.get(\"https://www.billboard.com/charts/year-end/\"+str(year)+\"/top-billboard-200-albums\")\n",
    "        data = url.content\n",
    "        all_albums = all_albums + process_chart(data,year)\n",
    "    \n",
    "    # Filter just the top 10 albums for each year and insert into final list of dictionaries\n",
    "    album_dicts = []\n",
    "    for album in all_albums:\n",
    "        if (album[\"rank\"] < 11):\n",
    "            album_dicts.append(album)\n",
    "            \n",
    "    print(\"Albums Scraped from Billboard.\")\n",
    "    \n",
    "    return(album_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Metacritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metacritic_movie_scraper(url):\n",
    "\n",
    "    \"\"\"Scrapes given metacritic.com url for the movie review information.\n",
    "    Returns a dictionary with number of user reviews, average user review, number of critic reviews, and critic score\"\"\"\n",
    "    \n",
    "    # Use splinter and beautiful soup to parse given url\n",
    "    with Browser(\"chrome\", **executable_path, headless=True) as browser:\n",
    "        browser.visit(url)\n",
    "        soup = bs(browser.html, \"html.parser\")\n",
    "\n",
    "    # Find number of reviews from users and critics\n",
    "    rev_count_strings = soup.find_all(\"span\", class_=\"based_on\")\n",
    "    user_rev_count = int(rev_count_strings[1].text.split(\" \")[2])\n",
    "    critic_rev_count = int(rev_count_strings[0].text.split(\" \")[2])\n",
    "\n",
    "    # Find review average from users and rating score from critics\n",
    "    review_soup = soup.find_all(\"a\", class_=\"metascore_anchor\")\n",
    "    user_rev_avg = float(review_soup[1].text)\n",
    "    critic_rev_score = int(review_soup[0].text)\n",
    "    \n",
    "    # Return dictionary of book information\n",
    "    movie_dict = {\"user_rev_count\": user_rev_count, \"user_rev_avg\": user_rev_avg, \"critic_rev_count\": critic_rev_count, \"critic_rev_score\": critic_rev_score}\n",
    "\n",
    "    return(movie_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metacritic_album_scraper(url):\n",
    "\n",
    "    \"\"\"Scrapes given metacritic.com url for the album review information.\n",
    "    Returns a dictionary with number of user reviews, average user review, number of critic reviews, and critic score\"\"\"\n",
    "    \n",
    "    # Use splinter and beautiful soup to parse given url   \n",
    "    with Browser(\"chrome\", **executable_path, headless=True) as browser:\n",
    "        browser.visit(url)\n",
    "        soup = bs(browser.html, \"html.parser\")\n",
    "    \n",
    "    # Find review average from users and rating score from critics\n",
    "    review_soup = soup.find_all(\"a\", class_=\"metascore_anchor\")\n",
    "    user_rev_avg = float(review_soup[1].text)\n",
    "    critic_rev_score = int(review_soup[0].text)\n",
    "\n",
    "    # Find number of user reviews\n",
    "    count_soup = soup.find(\"div\",class_=\"module reviews_module user_reviews_module\")\n",
    "    user_rev_count_string = count_soup.find(\"span\",class_=\"count\")\n",
    "    user_rev_count = int(user_rev_count_string.text)\n",
    "\n",
    "    # Find number of critic reviews\n",
    "    critic_rev_count_string = count_soup.find(\"span\",class_=\"count\")\n",
    "    critic_rev_count = int(critic_rev_count_string.text)\n",
    "\n",
    "    # Return dictionary of album information\n",
    "    album_dict = {\"user_rev_count\": user_rev_count, \"user_rev_avg\": user_rev_avg, \"critic_rev_count\": critic_rev_count, \"critic_rev_score\": critic_rev_score}\n",
    "  \n",
    "    return (album_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create list of dictionaries for top movies and music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape BoxOfficeMojo and Billboard Music for a list of dictionaries of the top 10 movies for 2008-2018\n",
    "movie_BOM_dicts = movie_scraper()\n",
    "album_Bill_dicts = album_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rank': 5, 'title': 'Mamma Mia', 'studio': 'Uni.', 'year': 2008}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_BOM_dicts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies Scraped from BoxOfficeMojo.\n",
      "Albums Scraped from Billboard.\n",
      "the-dark-knight scraped\n",
      "indiana-jones-and-the-kingdom-of-the-crystal-skull scraped\n",
      "kung-fu-panda scraped\n",
      "hancock scraped\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-24db6eee31d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Add review information to dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmovie_dicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetacritic_movie_scraper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{movie_query} scraped\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-39287a9b1551>\u001b[0m in \u001b[0;36mmetacritic_movie_scraper\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Find number of reviews from users and critics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mrev_count_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"span\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"based_on\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0muser_rev_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev_count_strings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mcritic_rev_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev_count_strings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Add review information from Metacritic to new list of dictionaries for top movies\n",
    "movie_dicts = []\n",
    "for movie in movie_BOM_dicts:\n",
    "    \n",
    "    # Create query url from dictionary values\n",
    "    movie_query = movie[\"title\"].replace(\" \", \"-\").lower()\n",
    "    movie_url = f\"https://www.metacritic.com/movie/{movie_query}/details\"\n",
    "\n",
    "    # Add review information to dictionary\n",
    "    movie_dicts.append(metacritic_movie_scraper(movie_url))\n",
    "    print(f\"{movie_query} scraped\")\n",
    "    \n",
    "# Add review information from Metacritic to new list of dictionaries for top music albums\n",
    "album_dicts = []\n",
    "for album in album_Bill_dicts:\n",
    "    # Create query url from dictionary values\n",
    "    title_query = album[\"title\"].replace(\" \", \"-\").lower()\n",
    "    artist_query = album[\"artist\"].replace(\" \", \"-\").lower()\n",
    "    album_url = f\"https://www.metacritic.com/music/{title_query}/{artist_query}\"\n",
    "    \n",
    "    # Add review information to dictionary\n",
    "    album_dicts.append(metacritic_album_scraper(album_url))\n",
    "    print(f\"{title_query} scraped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate mongo database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to mongo using pymongo to create local database\n",
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "# Create Top 10 database\n",
    "db = client.top_10_db\n",
    "\n",
    "# Create movies and albums collections\n",
    "movies = db.movies\n",
    "albums = db.albums\n",
    "\n",
    "# Insert top 10 movies and albums for 2008-2018\n",
    "# GRETEL - FIGURE OUT WHETHER WE WANT TO UPSERT\n",
    "db.movies.insert_many(movie_dict_list)\n",
    "db.albums.insert_many(album_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
